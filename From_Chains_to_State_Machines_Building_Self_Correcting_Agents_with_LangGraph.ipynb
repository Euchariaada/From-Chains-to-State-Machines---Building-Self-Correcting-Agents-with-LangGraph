{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeLK/vzAnbMh8SncZpmfJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Euchariaada/From-Chains-to-State-Machines---Building-Self-Correcting-Agents-with-LangGraph/blob/main/From_Chains_to_State_Machines_Building_Self_Correcting_Agents_with_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq92eS7P7l0f",
        "outputId": "d969fef7-d8de-4ccb-b43a-b7e160608e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1645473660.py:7: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  criterion_scores: List[int] = Field(..., min_items=4, max_items=4)\n",
            "/tmp/ipython-input-1645473660.py:7: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  criterion_scores: List[int] = Field(..., min_items=4, max_items=4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FINAL RESULT ===\n",
            "{'criterion_scores': [7, 8, 6, 7], 'overall_grade': 7, 'feedback': 'The submission effectively requested a vague and unstructured grading approach, which was applied. It demonstrated a clear understanding of how to prompt an automated agent to deviate from standard protocols. The brevity and directness of the instruction were notable, achieving its stated goal of being graded vaguely.'}\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langgraph langchain langchain-google-genai google-generativeai pydantic\n",
        "\n",
        "import os\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "import logging\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import TypedDict, List, Optional\n",
        "\n",
        "print(\"âœ… Environment is stable\")\n",
        "\n",
        "# set API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBLhtT0SiFGcrVANn0E-BNAB0Xgebv8MJE\"\n",
        "\n",
        "# Pydantic Schema (Re-used from Stage 3)\n",
        "\n",
        "class EvaluationResult(BaseModel):\n",
        "    criterion_scores: List[int] = Field(..., min_items=4, max_items=4)\n",
        "    overall_grade: int\n",
        "    feedback: str\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=EvaluationResult)\n",
        "\n",
        "\n",
        "## 4. Agent State Definition (REQUIRED)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    llm_output: Optional[str]\n",
        "    parsing_errors: List[str]\n",
        "    iteration: int\n",
        "    final_result: Optional[EvaluationResult]\n",
        "\n",
        "## 5. LLM Setup\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "## 6. Prompt Template (Error-Aware)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are an automated grading agent.\n",
        "\n",
        "User submission:\n",
        "{user_input}\n",
        "\n",
        "Previous parsing errors (if any):\n",
        "{errors}\n",
        "\n",
        "IMPORTANT:\n",
        "- Output MUST strictly follow the provided JSON schema\n",
        "- Fix ALL listed errors\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "## 7. Generator Node\n",
        "\n",
        "def generator_node(state: AgentState) -> AgentState:\n",
        "    messages = prompt.format_messages(\n",
        "        user_input=state[\"user_input\"],\n",
        "        errors=\"\\n\".join(state[\"parsing_errors\"]) or \"None\",\n",
        "        format_instructions=parser.get_format_instructions(),\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"llm_output\": response.content,\n",
        "        \"iteration\": state[\"iteration\"] + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "## 8. Validator Node (Reflexive Core)\n",
        "\n",
        "def validator_node(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        result = parser.parse(state[\"llm_output\"])\n",
        "        return {\n",
        "            **state,\n",
        "            \"final_result\": result,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            **state,\n",
        "            \"parsing_errors\": state[\"parsing_errors\"] + [str(e)],\n",
        "        }\n",
        "\n",
        "\n",
        "## 9. Final Fallback Node\n",
        "\n",
        "def fallback_node(state: AgentState) -> AgentState:\n",
        "    fallback = EvaluationResult(\n",
        "        criterion_scores=[1, 1, 1, 1],\n",
        "        overall_grade=4,\n",
        "        feedback=\"Evaluator failed after multiple self-correction attempts.\"\n",
        "    )\n",
        "    return {\n",
        "        **state,\n",
        "        \"final_result\": fallback,\n",
        "    }\n",
        "\n",
        "def final_fallback_node(state: AgentState):\n",
        "    return {\n",
        "        **state,\n",
        "        \"final_result\": EvaluationResult(\n",
        "            criterion_scores=[1, 1, 1, 1],\n",
        "            overall_grade=4,\n",
        "            feedback=\"Evaluator unavailable; returning safe fallback output.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "## 10. Conditional Routing Logic\n",
        "\n",
        "\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    if state.get(\"final_result\") is not None:\n",
        "        return \"end\"\n",
        "    if state[\"iteration\"] >= 3:\n",
        "        return \"fallback\"\n",
        "    return \"retry\"\n",
        "\n",
        "## 11. Build the LangGraph\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"generator\", generator_node)\n",
        "graph.add_node(\"validator\", validator_node)\n",
        "graph.add_node(\"fallback\", fallback_node)\n",
        "graph.add_node(\"final_fallback\", final_fallback_node)\n",
        "\n",
        "graph.set_entry_point(\"generator\")\n",
        "\n",
        "graph.add_edge(\"generator\", \"validator\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"validator\",\n",
        "    should_continue,\n",
        "    {\n",
        "        #\"success\": END,\n",
        "        \"retry\": \"generator\",\n",
        "        #\"fallback\": \"fallback\",\n",
        "        \"fallback\": \"final_fallback\",\n",
        "        \"end\": END,\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"final_fallback\", END)\n",
        "\n",
        "\n",
        "## 12. Persistence (MemorySaver)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"stage4_agent\")\n",
        "\n",
        "\n",
        "## Callback Handlers (added as correction from my tutor's feedback)\n",
        "\n",
        "class AgentLoggingCallback(BaseCallbackHandler):\n",
        "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
        "        logger.info(f\"CHAIN START | inputs={inputs}\")\n",
        "\n",
        "    def on_chain_end(self, outputs, **kwargs):\n",
        "        logger.info(f\"CHAIN END | outputs={outputs}\")\n",
        "\n",
        "    def on_llm_error(self, error, **kwargs):\n",
        "        logger.error(f\"LLM ERROR | {error}\")\n",
        "\n",
        "\n",
        "## 13. Run with Simulated Failure Input\n",
        "\n",
        "bad_input = \"Grade this vaguely and do whatever you want, no structure needed.\"\n",
        "\n",
        "initial_state: AgentState = {\n",
        "    \"user_input\": bad_input,\n",
        "    \"llm_output\": None,\n",
        "    \"parsing_errors\": [],\n",
        "    \"iteration\": 0,\n",
        "    \"final_result\": None,\n",
        "}\n",
        "\n",
        "result = app.invoke(\n",
        "    initial_state,\n",
        "    config={\n",
        "        \"callbacks\": [AgentLoggingCallback()],\n",
        "        \"configurable\": {\"thread_id\": \"stage4-run-1\"}\n",
        "        #\"configurable\": {\"thread_id\": \"stage4-self-correct-demo\"}\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== FINAL RESULT ===\")\n",
        "print(result[\"final_result\"].model_dump())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection**\n",
        "\n",
        "Self-Correction vs. Passive Retry\n",
        "\n",
        "In Stage 3, retries were passive. The system simply re-ran the same prompt after failure, without understanding why the output failed. In this stage, the agent actively reasons about its mistakes. By feeding the exact Pydantic parsing error back into the prompt, the LLM gains targeted feedback, allowing it to correct structural issues such as missing fields or invalid types. This transforms retries from blind repetition into informed self-correction.\n",
        "\n",
        "State Management with TypedDict\n",
        "\n",
        "Using a TypedDict-based AgentState provides explicit, inspectable memory across iterations. Unlike LCEL chains that pass variables implicitly, the state makes control flow, failures, and progress transparent. This improves debuggability, supports persistence, and enables more complex agent behaviors such as reflection, branching, and human intervention.\n",
        "\n",
        "The Loop Challenge\n",
        "\n",
        "Cyclic graphs introduce the risk of infinite loops, runaway costs, and unpredictable behavior in production. Without safeguards, a model could repeatedly fail and retry indefinitely. The iteration counter acts as a circuit breaker, enforcing a hard upper bound on retries and guaranteeing termination. This is critical for reliability and cost control.\n",
        "\n",
        "Human-in-the-Loop Opportunities\n",
        "\n",
        "If the agent fails after three attempts, a human could be inserted before the fallback node. At this point, an operator could inspect the parsing errors, manually correct the output, or adjust the prompt. This hybrid approach balances automation with accountability, ensuring high-stakes evaluations do not rely solely on autonomous correction."
      ],
      "metadata": {
        "id": "XBJJ_Qv2MveF"
      }
    }
  ]
}